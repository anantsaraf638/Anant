{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f8082b-698e-401a-880b-9d47d4dfc2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "# Or for TensorFlow backend: !pip install transformers tensorflow\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be59e325-04d3-43ba-a89d-eda1d445c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "--- Generating Text for Specific Topics ---\n",
      "\n",
      "Prompt: The alien spacecraft shimmered, a silent sentinel against the twilight sky. Its arrival marked a new era for humanity, one filled with both promise and profound uncertainty.\n",
      "Generated (SF 1): The alien spacecraft shimmered, a silent sentinel against the twilight sky. Its arrival marked a new era for humanity, one filled with both promise and profound uncertainty. I watched as my ship's hull collapsed in mid-flight to its side near some isolated canyon on Icy Mountain where an ancient relic of our past still resides: A colossal fragment known only by many people—a giant asteroid that was once used to carry valuable information into space from Earth orbit but became lost because it fell ill\n",
      "Generated (SF 2): The alien spacecraft shimmered, a silent sentinel against the twilight sky. Its arrival marked a new era for humanity, one filled with both promise and profound uncertainty. The human race must now face its own destiny in this world of chaos!\n",
      "For some reason I think it is impossible to know what caused them all; but if you are not careful enough people will probably turn out like that...<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "Prompt: The signing of the Declaration of Independence on July 4, 1776, was a pivotal moment in American history.\n",
      "Generated (History): The signing of the Declaration of Independence on July 4, 1776, was a pivotal moment in American history. At its height, it represented an unprecedented shift from imperialistic diplomacy to civil rights and equality for all Americans: \"In this country,\" wrote Thomas Jefferson during his speech at Georgetown University's Kennedy School (1907), \"[t]he people have taken their liberty by force; they are free.\"\n",
      "<|endoftext|>\n",
      "\n",
      "Prompt: The ancient redwood forest stood tall, a testament to centuries of silent growth.\n",
      "Generated (Nature): The ancient redwood forest stood tall, a testament to centuries of silent growth. It was one of the most treasured sites in this region and there were many legends about its mysterious presence that persisted for hundreds or thousands years after it disappeared from sight – including stories by several legendary explorers who told tales similar to these recounted on various islands back home.\"\n",
      "Aerial view: The tree is located across southern South Australia at Gwynedd Bay National Park (\n",
      "\n",
      "Prompt: Creativity is not merely an artistic pursuit; it is a fundamental human drive that fuels innovation in every field.\n",
      "Generated (Abstract): Creativity is not merely an artistic pursuit; it is a fundamental human drive that fuels innovation in every field. The basic problem with the idea of \"creative destruction\" lies at least as deep and pervasive within our culture as we are: In order to make any sort, there must be something wrong about what's happening—or else no one will see how creative work actually works (and thus just start looking). That doesn't mean I'm against creativity but rather those who insist on making things better for everyone by taking away from them certain freedoms\n",
      "\n",
      "Prompt for Experiment: A lone wolf howled at the full moon, its mournful cry echoing through the desolate wilderness.\n",
      "\n",
      "--- Higher Temperature (More Creative) ---\n",
      "Generated (High Temp): A lone wolf howled at the full moon, its mournful cry echoing through the desolate wilderness. When two wolves had been left behind to carry out their duties with such speed and ferocity as they scarcely knew what else could happen next would be enough for them to take on a heavy burden that neither of these might think worthy.\"\n",
      "\"Now,\" I said solemnly, \"if we want some small piece from you or your companions in this matter; but before leaving our place there's nothing too\n",
      "\n",
      "--- Lower Temperature (More Focused) ---\n",
      "Generated (Low Temp): A lone wolf howled at the full moon, its mournful cry echoing through the desolate wilderness. \"I am a beast of burden,\" said Rhea in her voice as she stood before them on their way to meet with another one of those who had come for shelter from wolves and bears over many years ago: \"You are my sister.\" She was not afraid; they were all men now… but I did remember that when we first met there would be no more than two or three people\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "model_name = \"gpt2\" \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Model moved to GPU.\")\n",
    "else:\n",
    "    print(\"Running on CPU.\")\n",
    "\n",
    "\n",
    "def generate_coherent_paragraph(prompt, model, tokenizer, max_length=150, temperature=0.7, top_k=50, top_p=0.95, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generates coherent paragraphs based on a given prompt using a GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The starting text for generation.\n",
    "        model: The loaded GPT-2 language model.\n",
    "        tokenizer: The loaded GPT-2 tokenizer.\n",
    "        max_length (int): Maximum length of the generated text (including prompt).\n",
    "        temperature (float): Controls randomness. Higher values make output more random.\n",
    "        top_k (int): Number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "        top_p (float): Cumulative probability threshold for nucleus sampling.\n",
    "        num_return_sequences (int): Number of independent sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated paragraphs (strings).\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "\n",
    "    \n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.2, \n",
    "        do_sample=True,         \n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id \n",
    "    )\n",
    "\n",
    "    generated_paragraphs = []\n",
    "    for i, generated_sequence in enumerate(output_sequences):\n",
    "        full_text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        generated_text = full_text[len(prompt):].strip()\n",
    "        generated_paragraphs.append(generated_text)\n",
    "    return generated_paragraphs\n",
    "\n",
    "\n",
    "print(\"--- Generating Text for Specific Topics ---\")\n",
    "\n",
    "\n",
    "prompt_sf = \"The alien spacecraft shimmered, a silent sentinel against the twilight sky. Its arrival marked a new era for humanity, one filled with both promise and profound uncertainty.\"\n",
    "print(f\"\\nPrompt: {prompt_sf}\")\n",
    "generated_sf = generate_coherent_paragraph(prompt_sf, model, tokenizer, max_length=100, temperature=0.8, num_return_sequences=2)\n",
    "for i, text in enumerate(generated_sf):\n",
    "    print(f\"Generated (SF {i+1}): {prompt_sf} {text}\")\n",
    "\n",
    "\n",
    "prompt_hist = \"The signing of the Declaration of Independence on July 4, 1776, was a pivotal moment in American history.\"\n",
    "print(f\"\\nPrompt: {prompt_hist}\")\n",
    "generated_hist = generate_coherent_paragraph(prompt_hist, model, tokenizer, max_length=120, temperature=0.7, top_k=40)\n",
    "print(f\"Generated (History): {prompt_hist} {generated_hist[0]}\")\n",
    "\n",
    "\n",
    "prompt_nature = \"The ancient redwood forest stood tall, a testament to centuries of silent growth.\"\n",
    "print(f\"\\nPrompt: {prompt_nature}\")\n",
    "generated_nature = generate_coherent_paragraph(prompt_nature, model, tokenizer, max_length=90, temperature=0.9, top_p=0.9)\n",
    "print(f\"Generated (Nature): {prompt_nature} {generated_nature[0]}\")\n",
    "\n",
    "\n",
    "prompt_abstract = \"Creativity is not merely an artistic pursuit; it is a fundamental human drive that fuels innovation in every field.\"\n",
    "print(f\"\\nPrompt: {prompt_abstract}\")\n",
    "generated_abstract = generate_coherent_paragraph(prompt_abstract, model, tokenizer, max_length=110, temperature=0.6)\n",
    "print(f\"Generated (Abstract): {prompt_abstract} {generated_abstract[0]}\")\n",
    "\n",
    "\n",
    "prompt_exp = \"A lone wolf howled at the full moon, its mournful cry echoing through the desolate wilderness.\"\n",
    "print(f\"\\nPrompt for Experiment: {prompt_exp}\")\n",
    "print(\"\\n--- Higher Temperature (More Creative) ---\")\n",
    "generated_exp_high_temp = generate_coherent_paragraph(prompt_exp, model, tokenizer, max_length=100, temperature=1.0)\n",
    "print(f\"Generated (High Temp): {prompt_exp} {generated_exp_high_temp[0]}\")\n",
    "\n",
    "print(\"\\n--- Lower Temperature (More Focused) ---\")\n",
    "generated_exp_low_temp = generate_coherent_paragraph(prompt_exp, model, tokenizer, max_length=100, temperature=0.5)\n",
    "print(f\"Generated (Low Temp): {prompt_exp} {generated_exp_low_temp[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1d972-66b7-4da3-8440-acfbceca9720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
